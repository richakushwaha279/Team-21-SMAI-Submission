{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Dividing the data file into quarter size\n",
    "\n",
    "Since the data is very huge (2.35 GB) and takes a lot of time to process, we create a chunk of file which is 1/4th the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_chunks(file_size):\n",
    "    chunk_start = 0\n",
    "    chunk_size = 0x20000\n",
    "    while chunk_start + chunk_size < file_size:\n",
    "        yield(chunk_start, chunk_size)\n",
    "        chunk_start += chunk_size\n",
    "    final_chunk_size = file_size - chunk_start\n",
    "    yield(chunk_start, final_chunk_size)\n",
    "\n",
    "def read_file_chunked(file_path):\n",
    "    with open(file_path) as file_:\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print file_size\n",
    "        print('File size: {}'.format(file_size))\n",
    "        progress = 0\n",
    "\n",
    "        for chunk_start, chunk_size in get_chunks(file_size):\n",
    "\n",
    "            file_chunk = file_.read(chunk_size)\n",
    "            f.write(file_chunk)\n",
    "            progress += len(file_chunk)\n",
    "            if(progress>=file_size/2):\n",
    "                break\n",
    "            print('{0} of {1} bytes read'.format(progress, file_size))\n",
    "\n",
    "f = open('prep.tsv', 'ab+')\n",
    "read_file_chunked('data.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Removing the Extra Columns from the Dataset\n",
    "\n",
    "Originally, the data has 6 fields :\n",
    "    <user_id> <timestamp> <artist_id> <artist_name> <song_id> <song_name>\n",
    "From this, we extract only 3 colums to create our MPS :\n",
    "    <user_id> <timestamp> <song>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('prep.tsv')\n",
    "f1 = open('newprep.tsv','w')\n",
    "for x in xrange(4781164):\n",
    "    s = f.readline()\n",
    "    l = s.split('\\t')\n",
    "    f1.write(l[0][5:]+'\\t'+l[1]+'\\t'+l[-1])\n",
    "f.close()\n",
    "f1.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Creating MPS from the Dataset\n",
    "\n",
    "Using the Pandas library, we create MPS from the user,timestamp,song values. Songs played between 800s duration are kept in one sequence. MPS with size less than 10 are discarded from the data (As given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('newprep.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=['UserID','TimeStamp'],ascending=[True,True],inplace=True)\n",
    "MPS = {}\n",
    "idnum = data.iloc[0]['UserID']\n",
    "song = data.iloc[0]['Song']\n",
    "ts = data.iloc[0]['TimeStamp']\n",
    "l = [song]\n",
    "for i in xrange(1,len(data)):\n",
    "    newid = data.iloc[i]['UserID']\n",
    "    tsnew = data.iloc[i]['TimeStamp']\n",
    "    newsong = data.iloc[i]['Song']\n",
    "    if newid == idnum:\n",
    "        d = pd.Timedelta(data.iloc[i]['TimeStamp']-data.iloc[i-1]['TimeStamp']).seconds\n",
    "        if d < 800:\n",
    "            l.append(data.iloc[i]['Song'])\n",
    "        else:\n",
    "            if idnum in MPS.keys():\n",
    "                MPS[idnum].append(l)\n",
    "            else:\n",
    "                MPS[idnum] = [l]\n",
    "            l = []\n",
    "            l.append(data.iloc[i]['Song'])\n",
    "    else:\n",
    "        if idnum in MPS.keys():\n",
    "            MPS[idnum].append(l)\n",
    "        else:\n",
    "            MPS[idnum] = [l]\n",
    "        idnum = newid\n",
    "        l = []\n",
    "        l.append(data.iloc[i]['Song'])\n",
    "if l not in MPS[idnum]:\n",
    "    if idnum in MPS.keys():\n",
    "        MPS[idnum].append(l)\n",
    "    else:\n",
    "        MPS[idnum] = [l]\n",
    "for user in MPS:\n",
    "    for ele in MPS[user]:\n",
    "        if len(ele) < 10:\n",
    "            MPS[user].remove(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Creating a Song2Vec Model using the MPS Data\n",
    "\n",
    "Now we use the Word2Vec Model implemented in gensim library to create our song2vec model. Here we dont account the songs that have been played less than 10 times (as described in paper) (by setting the 'min_count' variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "f = open('MPS.txt','r')\n",
    "for line in f:\n",
    "    l = line.split(\"\\t\")\n",
    "    l[1] = l[1].replace('nan','')\n",
    "    l[1] = l[1].replace(', ,',',')\n",
    "    templist = eval(l[1])\n",
    "    if len(templist) < 10:\n",
    "        continue\n",
    "    corpus.append(templist)\n",
    "print len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "135363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2vec = w2v.Word2Vec(sg=1,seed=1,size=100,min_count=10,window=5)\n",
    "song2vec.build_vocab(corpus)\n",
    "# Preprocess the songs that contain unprintable unicode characters\n",
    "vocabulary1=list(song2vec.wv.vocab)\n",
    "print len(vocabulary1)\n",
    "vocabulary1=list(song2vec.wv.vocab)\n",
    "vocabulary1=[x.encode('UTF8') for x in vocabulary1]\n",
    "vocabulary=[]\n",
    "for word in vocabulary1:\n",
    "    if re.match('[A-Za-z0-9\\'\\?\\&\\)\\(\\+\\!\\-\\*\\.\\, ]+$',word):\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary.sort()\n",
    "print vocabulary\n",
    "print len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2vec.train(corpus,total_examples=len(corpus),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(32145580, 40475800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print song2vec.most_similar(u'The Boy Looked At Johnny',topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(u'Radio America', 0.8883671164512634), (u'Music When The Lights Go Out (Demo)', 0.8557537198066711), (u'Cola Queen', 0.8539921045303345), (u'Death On The Stairs (New Recording)', 0.8484163284301758), (u'Babyshambles1', 0.8476840257644653), (u'Albion 1', 0.8459166884422302), (u'The Man Who Would Be King', 0.8445132374763489), (u'The Road To Ruin 2', 0.8408887386322021), (u'Skag & Bone Man', 0.8405758142471313), (u'That Bowery Song', 0.8390020132064819)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print song2vec.most_similar('Gang Of Gin',topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(u'Fuck Forever (Clean)', 0.86831134557724), (u'Killamangiro', 0.830515444278717), (u'Lust Of The Libertines', 0.8216589689254761), (u'What Katy Did Next', 0.8154677152633667), (u'\\xc1 Rebours', 0.8092206120491028), (u'Fuck Forever (Original)', 0.8060870170593262), (u'Beg Steal Or Borrow', 0.8037663102149963), (u'Do You Know Me', 0.8007555603981018), (u'8 Dead Boys', 0.7971910834312439), (u'Fixing Up To Go', 0.7954947352409363)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find top n songs for any song, just put the name in song name\n",
    "song_name = \"\"\n",
    "l = song2vec.most_similar(song_name,topn=10)\n",
    "print \"Top 10 Songs are: \"\n",
    "for ele in l:\n",
    "    name,sim = ele[0],ele[1]\n",
    "    print name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Creating Playcount Matrix using MPS Data\n",
    "\n",
    "Creating a matrix containing how many times a particular song was played by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_count = np.zeros((249,len(vocabulary)))\n",
    "f = open('MPS.txt','r')\n",
    "for line in f:\n",
    "    l1 = line.split(\"\\t\")\n",
    "    userno = int(l1[0])\n",
    "    l1[1] = l1[1].replace('nan','')\n",
    "    l1[1] = l1[1].replace(', ,',',')\n",
    "    songs = eval(l1[1])\n",
    "    if len(songs) < 10:\n",
    "        continue\n",
    "    print \"User Number :  {0}\".format(userno)\n",
    "    songs=[x.encode('UTF8') for x in songs]\n",
    "    for song in songs:\n",
    "        if song in vocabulary:\n",
    "            index = vocabulary.index(song)\n",
    "            play_count[userno-1][index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Creating the Rating Matrix\n",
    "\n",
    "We use the playcount matrix to create the rating matrix using the method described in [Choi et. al., 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP = np.zeros(play_count.shape)\n",
    "R = np.zeros(play_count.shape)\n",
    "for u in xrange(len(play_count)):\n",
    "    for i in xrange(len(play_count[u])):\n",
    "        temp = np.sum(play_count[u,:])\n",
    "        temp1 = np.sum(play_count[:,i])\n",
    "        AP[u,i] = np.log((temp/temp1)+1)\n",
    "    print \"User {0} Done!\".format(u)\n",
    "for u in xrange(len(play_count)):\n",
    "    for i in xrange(len(play_count[u])):\n",
    "        R[u,i] = (5*(AP[u,i]/np.max(AP[:,i])))\n",
    "    print \"User {0} Done!\".format(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Matrix Factorization on Ratings Matrix\n",
    "\n",
    "Now we apply standard matrix factorization on our ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in xrange(steps):\n",
    "        for i in xrange(len(R)):\n",
    "            for j in xrange(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in xrange(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in xrange(len(R)):\n",
    "            for j in xrange(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in xrange(K):\n",
    "                        e = e + (beta/2) * ( pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters\n",
    "N = len(R)\n",
    "M = len(R[0])\n",
    "K = 100   \n",
    "iterations = 20\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "nP, nQ = matrix_factorization(R, P, Q, K)\n",
    "RHat = np.dot(nP,nQ.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Applying Biased Matrix Factorization on Ratings Matrix\n",
    "\n",
    "Now to compare with the previously applied MF, we apply Biased MF on the ratings Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "\n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty\n",
    "        entries in a matrix.\n",
    "\n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "\n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "\n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "\n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "\n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "\n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MF(R, K=2, alpha=0.1, beta=0.01, iterations=20)\n",
    "training_process = mf.train()\n",
    "RHatBiased = mf.full_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Applying the given method in paper to the rating matrix\n",
    "\n",
    "Using the given loss function in the paper and using the song-song similarities obtained using song2vec to regularize the matrix factorization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_items = R.shape\n",
    "b_u_final = np.zeros(num_users)\n",
    "b_i_final = np.zeros(num_items)\n",
    "b_final = np.mean(R[np.where(R != 0)])\n",
    "P_ = []\n",
    "Q_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse():\n",
    "    \"\"\"\n",
    "    A function to compute the total mean square error\n",
    "    \"\"\"\n",
    "    xs, ys =  R.nonzero()\n",
    "     b_final +  b_u_final[:,np.newaxis] +  b_i_final[np.newaxis:,] +  P_.dot(Q_.T)\n",
    "    error = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        error += pow( R[x, y] - predicted[x, y], 2)\n",
    "    return np.sqrt(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global samples, b_u_final, b_i_final, b_final, P_, Q_\n",
    "    num_users, num_items = R.shape\n",
    "    lam = 0.1\n",
    "    P = np.random.normal(scale=1./ K, size=( num_users,  K))\n",
    "    Q = np.random.normal(scale=1./ K, size=( num_items,  K))\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_items)\n",
    "    b = np.mean(R[np.where(R != 0)])\n",
    "    samples = [\n",
    "        (i, j,  R[i, j])\n",
    "        for i in range(num_users)\n",
    "        for j in range(num_items)\n",
    "        if  R[i, j] > 0]\n",
    "    training_process = []\n",
    "    for i in range(iterations):\n",
    "        np.random.shuffle( samples)\n",
    "        for l, i, r in samples:\n",
    "            prediction =  b +  b_u[l] +  b_i[i] +  P[l, :].dot( Q[i, :].T)\n",
    "            e = (r - prediction)\n",
    "            # Update biases\n",
    "            b_u[i] +=  alpha * (e -  beta *  b_u[i])\n",
    "            b_i[j] +=  alpha * (e -  beta *  b_i[j])\n",
    "            # Update P Matrix\n",
    "            P[i, :] +=  alpha * (e *  Q[j, :] -  beta *  P[i,:])\n",
    "            # Updation of Q based on k nearest neighbours.\n",
    "            Qi = Q[i,:]\n",
    "            val = e\n",
    "            song_name = vocabulary[i]\n",
    "            top5 = song2vec.most_similar(song_name,topn=5)\n",
    "            t1 = top5[0][1]\n",
    "            Qj = Q[vocabulary.index(top[0][0]),:]\n",
    "            t2 = np.dot(Qi,Qj)\n",
    "            t3 = (t1-t2)*Qj + lam * Qj\n",
    "            sub = t3\n",
    "            for j in xrange(1,5):\n",
    "                t1 = top[j][1]\n",
    "                Qj = Q[vocabulary.index(top[j][0]),:]\n",
    "                t2 = np.dot(Qi,Qj)\n",
    "                t3 = (t1-t2)*Qj + lam * Qj\n",
    "                sub += t3\n",
    "            delQi = val - sub\n",
    "            Qi = Qi - alpha * (delQi)\n",
    "            Q[i,:] = Qi\n",
    "        mse1 =  mse()\n",
    "        training_process.append((i, mse1))\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "    b_final = b\n",
    "    b_u_final = b_u\n",
    "    b_i_final = b_i\n",
    "    Q_ = Q\n",
    "    P_ = P\n",
    "    return training_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = train()\n",
    "RHatPaper = np.dot(P_,Q_.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
